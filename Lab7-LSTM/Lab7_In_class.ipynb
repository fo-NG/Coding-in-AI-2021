{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Lab8_In_class.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jomGrnTqvzY2"
      },
      "source": [
        "# Lab 8: Training Deep Recurrent Neural Network - Part 2\n",
        "\n",
        "Name1, Student's ID1<br>\n",
        "\n",
        "## Lab Instruction - Language Modelling and Text Classification\n",
        "\n",
        "In this lab, you will learn to train a deep recurrent neural network using LSTM with the Keras library using the Tensorflow backend. Your task is to implement the natural language modelling and text generation.\n",
        "\n",
        "```\n",
        "alice_in_wonderland.txt\n",
        "```\n",
        "\n",
        "In class will use alice_in_wonderland as a text file. Then, you will train your language model using RNN-LSTM. \n",
        "\n",
        "\n",
        "\n",
        "- Language model (in Thai): http://bit.ly/language_model_1\n",
        "- Tutorial on how to create a language model (in English): https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
        "\n",
        "To evaluate the model, the perplexity measurement is used: https://stats.stackexchange.com/questions/10302/what-is-perplexity\n",
        "\n",
        "Last, fine-tune your model. You have to try different hyperparameter or adding more data. Discuss your result.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoJBrLKvzY6"
      },
      "source": [
        "#### 1. Load your data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcWCFsnfvzY7"
      },
      "source": [
        "# Import require library\n",
        "from keras import *\n",
        "from keras.preprocessing import text\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import _utils as fn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLrjxZrNvzZA"
      },
      "source": [
        "# Load data\n",
        "import csv\n",
        "\n",
        "# Load data\n",
        "file = open(\"/content/alice_in_wonderland.txt\",\"r\",encoding=\"utf8\", errors='ignore')\n",
        "raw_text = file.read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StYO4DI-vzZE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "be1ba59b-0f25-4d00-dd6d-a0e275cdeeb2"
      },
      "source": [
        "raw_text[:200]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'CHAPTER I.\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was rea'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-9U1szlRb1q"
      },
      "source": [
        "chars = sorted(list(set(raw_text)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuaAvGjnRkE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6207ae-966c-4400-a149-f9008162513c"
      },
      "source": [
        "print(\"Total characters: \", len(chars))\n",
        "print(\"Total word: \", len(raw_text.split()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters:  82\n",
            "Total word:  29371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksGjaIJvzZJ"
      },
      "source": [
        "#### 2. Data Preprocessing \n",
        "\n",
        "*Note that only story will be used as a dataset, footnote and creddit are not include.*\n",
        "\n",
        "The symbol '\\n' is indicated the end of the line ``<EOS>``, which is for our model to end the sentence here.\n",
        "\n",
        "To create a corpus for your model. The following code is can be used:</br>\n",
        "*Note that other techniques can be used*\n",
        "\n",
        "```python\n",
        "# cut the text in semi-redundant sequences of maxlen characters.\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "```\n",
        "\n",
        "The code loop through the data from first word to the last word. The maxlen define a next n word for a model to predict.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj-YPtYsvzZK"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkIyzqV0RyJy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9d26846b-bded-4acf-86b0-d65176a61536"
      },
      "source": [
        "# Adding end of string symbol use .replace   to replace data_text with  [  \\n\\n', \" <EOS> \" ]\n",
        "raw_text = raw_text.replace('\\n\\n', \" <EOS> \")\n",
        "raw_text[:200]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'CHAPTER I.\\nDown the Rabbit-Hole <EOS> \\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister wa'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKE389h4vzZO"
      },
      "source": [
        "# Preprocessing \n",
        "# Create corpus & Vectorization\n",
        "\n",
        "#Preprocessing \n",
        "# Create corpus & Vectorization\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "\n",
        "# basic cleanup\n",
        "corpus = raw_text.lower().split(\"\\n\")\n",
        "\n",
        "# tokenization\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "# Pre padding \n",
        "input_sequences = np.array(sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "# One-hot label\n",
        "label = keras.utils.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07zFy2jotl1F",
        "outputId": "36a23194-881d-42f6-82ae-90e6f4dc7ab6"
      },
      "source": [
        "print('Max sequence len: %s' % max_sequence_len)\n",
        "print('Total word len: %s' % total_words)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence len: 112\n",
            "Total word len: 3162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpPDrYsNtgX8",
        "outputId": "48d56216-fe6c-422c-c7f1-8fc29042da33"
      },
      "source": [
        "n_gram_sequence[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3160"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50nrDM_gtje1",
        "outputId": "99669d54-fc69-405b-9dab-06b22fb33bc3"
      },
      "source": [
        "print(predictors[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0 330]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZzUBdMctjgx",
        "outputId": "f27d8da5-d125-4738-f26d-b95080ea9af1"
      },
      "source": [
        "print(label[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ry6hn0vzZc"
      },
      "source": [
        "#### 3. Language Model\n",
        "\n",
        "Define RNN model using LSTM and word embedding representation</br>\n",
        "We will used perplexity as a metrics\n",
        "\n",
        "```python\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = keras.backend.pow(2.0, cross_entropy)\n",
        "    return perplexity\n",
        "```\n",
        "\n",
        "To used custom metrics function > https://keras.io/metrics/\n",
        "\n",
        "For a loss function `categorical_crossentropy` is used, any optimzation method can be applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upTYjfZNvzZc"
      },
      "source": [
        "from keras.layers import Embedding \n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout \n",
        "from keras.layers import Dense\n",
        "import keras.backend "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDOBkmNYvzZf"
      },
      "source": [
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = keras.backend.pow(2.0, cross_entropy)\n",
        "    return perplexity"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BFD82lMTTfQ"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAgBOsbrvzZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de1c916-2b09-4c33-8acd-7b3aca2049c8"
      },
      "source": [
        "\n",
        "# Define your model\n",
        "# Used Word Embedding \n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Embedding(total_words, 512,input_length=max_sequence_len-1,name='Embedding'))\n",
        "model.add(layers.LSTM(512, kernel_initializer = 'he_normal',\n",
        "                      dropout=0.3,\n",
        "                      return_sequences=True,\n",
        "                     name='LSTM1'))\n",
        "model.add(layers.LSTM(256, kernel_initializer = 'he_normal',\n",
        "                     dropout=0.3,\n",
        "                     name='LSTM2'))\n",
        "model.add(layers.Dense(total_words, activation='softmax',name='Output'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=[perplexity])\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Embedding (Embedding)        (None, 111, 512)          1618944   \n",
            "_________________________________________________________________\n",
            "LSTM1 (LSTM)                 (None, 111, 512)          2099200   \n",
            "_________________________________________________________________\n",
            "LSTM2 (LSTM)                 (None, 256)               787456    \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 3162)              812634    \n",
            "=================================================================\n",
            "Total params: 5,318,234\n",
            "Trainable params: 5,318,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65pM7pqNvzZm"
      },
      "source": [
        "# Define your model\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[ perplexity])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8SYfwTCvzZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ce3dfe-a9a5-4cb1-d3f9-e543e46dc887"
      },
      "source": [
        "history = model.fit(predictors, label,batch_size=32, epochs=10)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "893/893 [==============================] - 92s 94ms/step - loss: 6.1471 - perplexity: 331.1375\n",
            "Epoch 2/10\n",
            "893/893 [==============================] - 83s 93ms/step - loss: 5.6086 - perplexity: 211.9764\n",
            "Epoch 3/10\n",
            "893/893 [==============================] - 83s 93ms/step - loss: 5.2417 - perplexity: 141.0023\n",
            "Epoch 4/10\n",
            "893/893 [==============================] - 83s 93ms/step - loss: 4.9527 - perplexity: 104.3594\n",
            "Epoch 5/10\n",
            "893/893 [==============================] - 83s 93ms/step - loss: 4.6903 - perplexity: 78.2347\n",
            "Epoch 6/10\n",
            "893/893 [==============================] - 83s 93ms/step - loss: 4.4416 - perplexity: 58.8324\n",
            "Epoch 7/10\n",
            "893/893 [==============================] - 83s 94ms/step - loss: 4.2123 - perplexity: 45.6957\n",
            "Epoch 8/10\n",
            "893/893 [==============================] - 84s 94ms/step - loss: 3.9882 - perplexity: 36.2882\n",
            "Epoch 9/10\n",
            "893/893 [==============================] - 83s 93ms/step - loss: 3.7700 - perplexity: 29.6776\n",
            "Epoch 10/10\n",
            "893/893 [==============================] - 83s 93ms/step - loss: 3.5675 - perplexity: 24.8904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uZwRT6RvzZu"
      },
      "source": [
        "#### 4. Evaluate your model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g4DkVi3vzZv"
      },
      "source": [
        "\n",
        "# Create a function to evaluate your model using perplexity measurment (You can try adding other measurements as well)\n",
        "def evaluate_result(features, label, model ):\n",
        "    model.evaluate(features, label)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6-pIYam3ACT",
        "outputId": "602eddb0-e0bd-44dc-a287-7c2e840fb94d"
      },
      "source": [
        "evaluate_result(predictors, label, model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "893/893 [==============================] - 32s 35ms/step - loss: 3.1744 - perplexity: 17.2101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCU3FZjsvzZy"
      },
      "source": [
        "#### 5. Text generating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAd1Kdl3vzZz"
      },
      "source": [
        "\n",
        "    \n",
        "def generate_text(seedtext, next_words, max_sequence_len, model):\n",
        "  for j in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seedtext])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    #predicted = model.predict_classes(token_list, verbose=0)\n",
        "    predict_x=model.predict(token_list) \n",
        "    predicted =np.argmax(predict_x,axis=1)\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "    seedtext +=\" \" + output_word\n",
        "  return seedtext"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC3AVscmvzZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0018b55d-1574-4159-f033-8aaad2890d80"
      },
      "source": [
        "# generate your sample text\n",
        "\n",
        "seed_text = input('Enter your start sentence:')\n",
        "#generate_text , Input , Num_next_word,Max_sequence,Model\n",
        "gen_text = generate_text(seed_text,10,max_sequence_len,model)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your start sentence:I am\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-ZblD2rvZFnF",
        "outputId": "45ca21e6-5dd9-4234-d947-b7e5285af5b4"
      },
      "source": [
        "gen_text"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am i must be a little girl said the king eos'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaOPVnw1ZFos"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLz1ABUgvzaI"
      },
      "source": [
        "### More on Natural language Processing and Language model\n",
        "1. https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e \n",
        "2. https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b\n",
        "3. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "**Music generates by RNN**\n",
        "https://soundcloud.com/optometrist-prime/recurrence-music-written-by-a-recurrent-neural-network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SscOleVkXxiG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkhYcG9vXxjq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}